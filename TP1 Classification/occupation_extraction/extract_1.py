"""The goal of this lab is to classify Wikipedia abstracts about people by their professions. For example, the professions of Elvis Presley are "singer" and "actor".

=== Input ===

The input for training is a file wiki-train.json, which contains Wikipedia abstracts in the following form:
   {"title": "George_Washington",
    "summary": "George Washington was one of the ..."
    "occupations": ["yago:politician"]}

The input for testing is a file wiki-test.json, which contains Wikipedia abstracts of the same shape without the occupations:

   {"title": "Douglas_Adams",
    "summary": "Douglas Noel Adams was ..."}

=== Output ===

The output shall be a JSON file that assigns each Wikipedia abstract to a set of occupations:
   {"title": "Douglas_Adams",
    "occupations": ["Q36180", "Q28389"]}

We provide a gold standard of this form for the development input file.

=== Datasets ===

We provide 3 datasets:
1) a training dataset, which has the labels
2) a development dataset, which has the labels
3) a testing dataset, which does not have the labels, and which we use for grading

=== What to do ===

Adapt the method create_model(), so that it creates a neural network model that classifies the sentence.
There is no need to modify the other parts of the code -- although you are allowed to do so.

=== Suggestions ===
1) Select a suitable theta value
Reference: held-out set, cross validation, grid search...

2) Use pre-trained embeddings
reference: word2vector, GloVe, FastText...

3) Add extra features
reference: stop words, part-of-speech...

4) Try other neural networks
reference: CNN, RNN, Attention, Transformer

5) Avoid overfitting
reference: regularization, dropout...

6) Adjust other parameters
reference: learning rate, batch_size, epoch, layer's dimension

==== Submission ===

1) Take your code, any necessary resources to run the code, and the output of your code on the test dataset (no need to put the other datasets!)
2) ZIP these files in a file called firstName_lastName.zip
3) submit it here before the deadline announced during the lab:

https://www.dropbox.com/request/zwBcRYj17giDjCyPqFQM

"""
import sys
import subprocess
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Install necessary modules
file1 = open('requirements.txt', 'r')
requirements = file1.readlines()
for req in requirements:
    reqs = subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', req.strip("\n"), '--quiet'])


# Import necessary modules
import gzip
import json
import nltk
from tqdm import tqdm
import numpy as np


# Import custom functions wrote by us
from utils import load_vocabulary, read_dataset, get_label, f1_score, load_data, gen_vocabulary

# Import functions from keras
from keras.layers import Input, Embedding, Dense
from keras.models import Model, load_model
import keras.backend as K
from keras import Sequential
from keras.layers.pooling import GlobalAveragePooling1D
nltk.download('punkt')

# input files
# [ train_file ] is a training dataset that contains 266K samples.
# [ test_file ] is a testing dataset that contains 200K samples. You can test your model based on this file.
# [ predict_file ] is a predicting dataset that contains 201K samples. Each sample in this file does not have occupation labels.

train_file = 'wiki-train.json.gz'
test_file = 'wiki-dev.json.gz'
predict_file = 'wiki-test.json.gz'

# output files
# [ vocab_file ] has a word vocabulary that defines which words participate in this task.
# The default vocabulary is generated by our methods from training dataset,
# but you can create it in a way you like.
# [ model_file ] is used for store your trained model
# [ result_file ] is file that stores your predicted occupations.
# (This is the file you have to submit, once you ran on the test dataset)

vocab_file = 'vocab.txt'
model_file = 'my_model.h5'
result_file = 'result.json'

# Hyper-parameters: You don't have to change these, but you can.
# [ embedding_dimension ] the dimensions of word embeddings
# [ maximal_sentence_length ] the maximum length of each sentence
# [ number_of_labels ] the number of occupations
# [ epochs ] training epochs. Adjust this parameters to avoid overfitting and underfitting.
# [ batch_size ] the number of samples. It determines how many samples would be fed into your model.
# The size of this parameter also depends on how good your hardware is.
# [ theta ] A threshold to determine whether to assign a specific occupation label given a input sample.
# A suitable theta value will help your model

embedding_dimension = 200
maximal_sentence_length = 100
number_of_labels = 20
epochs = 3
batch_size = 32
theta = 0.1


# YOUR CODE GOES HERE
def create_model(vocab):
    '''
    :param vocab: a vocabulary dictionary which looks like {'python':0, 'java':1 ......}
    :return:
    '''
    model = Sequential()
    # initialized word embeddings randomly
    model.add(Embedding(len(vocab), embedding_dimension, input_length=maximal_sentence_length))
    # average all words
    model.add(GlobalAveragePooling1D())
    # two layer densely-connected NNs
    # # different parameters of a dense layer can have different performances
    # # here, we let the number of units equal 32, and use a relu activation.
    model.add(Dense(32, activation='relu'))
    # for final prediction
    model.add(Dense(number_of_labels, activation='sigmoid'))
    # compile the model
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])

    return model


def train(debug):
    '''
    train your model.
    :param debug:whether to use a small fraction of samples
    :return:
    '''

    # prepare data
    vocab_to_id = load_vocabulary(vocab_file)
    data_x, data_y = read_dataset(train_file, vocab_to_id, maximal_sentence_length, debug=debug)

    # create a model
    model = create_model(vocab_to_id)
    model.summary()

    # train
    print('start to train, data size = {a}'.format(a=len(data_x)))
    model.fit(data_x, data_y, validation_split=0.10, epochs=epochs, batch_size=batch_size)

    # save model
    model.save(model_file)


def evaluate_on_dev(debug):
    '''
    evaluare your model on the development dataset.

    :param debug:whether to use a small fraction of samples
    :return:
    '''

    # prepare data
    vocab_to_id = load_vocabulary(vocab_file)
    data_x, data_y = read_dataset(test_file, vocab_to_id, maximal_sentence_length, debug=debug)
    raw_samples = list(load_data(test_file))
    print('start to do validation, data size = {a}'.format(a=len(data_x)))
    _, id_to_labels = get_label()
    pred_labels, true_labels = list(), list()

    # load model
    model = load_model(model_file)

    # predict each sample
    for summary, label, raw in zip(data_x, data_y, raw_samples):
        result = model.predict(np.array([summary]))[0]
        pred = set([id_to_labels[i] for i, prob in enumerate(result) if prob > theta])
        true = set([id_to_labels[index] for index, e in enumerate(label) if e == 1])
        pred_labels.append(pred)
        true_labels.append(true)

        # print wrong prediction
        print('Title:' + raw.title)
        wrong_occupations = pred - true
        if len(wrong_occupations) > 0:
            print('[ wrong prediction ] this person does not have the occupations:{a}'.format(a=wrong_occupations))
        missing_occupations = true - pred
        if len(missing_occupations) > 0:
            print('[ missing prediction ] your prediction miss the occupations:{b}'.format(b=missing_occupations))
        print('---------------------------')

    # calculate metrics
    f1, precision, recall = f1_score(true_labels, pred_labels)
    print('result on validation set, f1 : {a}, precision : {b}, recall : {c}.'.
          format(a=f1, b=precision, c=recall))


def predict_on_test(debug):
    '''
    :param debug: whether to use a small fraction of samples
    :return:
    '''

    # prepare data
    _, id_to_labels = get_label()
    vocab_to_id = load_vocabulary(vocab_file)
    model = load_model(model_file)
    datax, _, = read_dataset(predict_file, vocab_to_id, maximal_sentence_length, debug)
    raw_samples = list(load_data(predict_file))

    # predict
    r_f = open(result_file, 'w', encoding='utf8')
    for data, raw_sample in tqdm(zip(datax, raw_samples)):
        result = model.predict(np.array([data]))[0]
        pred = [id_to_labels[i] for i, prob in enumerate(result) if prob > theta]
        r_f.write(json.dumps({
                            'title': raw_sample.title,
                            'occupations': pred
                        }) + "\n")


# create a vocabulary file if does not exist
if not os.path.exists(vocab_file):
    gen_vocabulary(train_file, vocab_file)

# train & evaluate & predict
# note: the switch 'debug' is True means only using a small fraction of samples, which can save time to debug your code.
# Change 'debug' to False when your want to train and test on all samples.
debug = True
train(debug=debug)
evaluate_on_dev(debug=debug)
predict_on_test(debug=debug)
