{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The goal of this lab is to classify Wikipedia abstracts about people by their professions. For example, the professions of Elvis Presley are \"singer\" and \"actor\".\n",
    "\n",
    "=== Input ===\n",
    "\n",
    "The input for training is a file wiki-train.json, which contains Wikipedia abstracts in the following form:\n",
    "   {\"title\": \"George_Washington\",\n",
    "    \"summary\": \"George Washington was one of the ...\"\n",
    "    \"occupations\": [\"yago:politician\"]}\n",
    "\n",
    "The input for testing is a file wiki-test.json, which contains Wikipedia abstracts of the same shape without the occupations:\n",
    "\n",
    "   {\"title\": \"Douglas_Adams\",\n",
    "    \"summary\": \"Douglas Noel Adams was ...\"}\n",
    "\n",
    "=== Output ===\n",
    "\n",
    "The output shall be a JSON file that assigns each Wikipedia abstract to a set of occupations:\n",
    "   {\"title\": \"Douglas_Adams\",\n",
    "    \"occupations\": [\"Q36180\", \"Q28389\"]}\n",
    "\n",
    "We provide a gold standard of this form for the development input file.\n",
    "\n",
    "=== Datasets ===\n",
    "\n",
    "We provide 3 datasets:\n",
    "1) a training dataset, which has the labels\n",
    "2) a development dataset, which has the labels\n",
    "3) a testing dataset, which does not have the labels, and which we use for grading\n",
    "\n",
    "=== What to do ===\n",
    "\n",
    "Adapt the method create_model(), so that it creates a neural network model that classifies the sentence.\n",
    "There is no need to modify the other parts of the code -- although you are allowed to do so.\n",
    "\n",
    "=== Suggestions ===\n",
    "1) Select a suitable theta value\n",
    "Reference: held-out set, cross validation, grid search...\n",
    "\n",
    "2) Use pre-trained embeddings\n",
    "reference: word2vector, GloVe, FastText...\n",
    "\n",
    "3) Add extra features\n",
    "reference: stop words, part-of-speech...\n",
    "\n",
    "4) Try other neural networks\n",
    "reference: CNN, RNN, Attention, Transformer\n",
    "\n",
    "5) Avoid overfitting\n",
    "reference: regularization, dropout...\n",
    "\n",
    "6) Adjust other parameters\n",
    "reference: learning rate, batch_size, epoch, layer's dimension\n",
    "\n",
    "==== Submission ===\n",
    "\n",
    "1) Take your code, any necessary resources to run the code, and the output of your code on the test dataset (no need to put the other datasets!)\n",
    "2) ZIP these files in a file called firstName_lastName.zip\n",
    "3) submit it here before the deadline announced during the lab:\n",
    "\n",
    "https://www.dropbox.com/request/zwBcRYj17giDjCyPqFQM\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary modules\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "#file1 = open('requirements.txt', 'r')\n",
    "#requirements = file1.readlines()\n",
    "#for req in requirements:\n",
    "#    reqs = subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', req.strip(\"\\n\"), '--quiet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell imports modules necessary to run this lab.\n",
    "\"\"\"\n",
    "import gzip\n",
    "import json\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell provides some basic function to extracting occupations.\n",
    "There is no need to modify this file unless you want.\n",
    "\"\"\"\n",
    "\n",
    "class InputSample(object):\n",
    "    def __init__(self, title, summary, occupation):\n",
    "        self.title = title\n",
    "        self.summary = summary\n",
    "        self.occupation = occupation\n",
    "\n",
    "\n",
    "def pad_sentence(sentence, max_len):\n",
    "    '''\n",
    "    make all sentences have the same length\n",
    "    :param sentence:\n",
    "    :param max_len:\n",
    "    :return:\n",
    "    '''\n",
    "    seg_id = pad_sequences([sentence], maxlen=max_len, padding='post')\n",
    "    return seg_id[0]\n",
    "\n",
    "\n",
    "def get_label():\n",
    "    occupations = [\n",
    "        'yago:Politician',\n",
    "        'yago:Researcher',\n",
    "        'yago:Football_player',\n",
    "        'yago:Writer',\n",
    "        'yago:Actor',\n",
    "        'yago:Painter',\n",
    "        'yago:Journalist',\n",
    "        'yago:University_teacher',\n",
    "        'yago:Singer',\n",
    "        'yago:Poet',\n",
    "        'yago:Composer',\n",
    "        'yago:Military_personnel',\n",
    "        'yago:Lawyer',\n",
    "        'yago:Film_actor',\n",
    "        'yago:Businessperson',\n",
    "        'yago:Historian',\n",
    "        'yago:Musician',\n",
    "        'yago:Film_director',\n",
    "        'yago:Screenwriter',\n",
    "        'yago:Physician'\n",
    "    ]\n",
    "\n",
    "    labels = {occ_id: index for index, occ_id in enumerate(occupations)}\n",
    "    id_to_labels = {index: occ_id for index, occ_id in enumerate(occupations)}\n",
    "    return labels, id_to_labels\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    '''\n",
    "    load original data\n",
    "    :param filename:\n",
    "    :return:\n",
    "    '''\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            people = json.loads(line)\n",
    "            occ_key = 'occupations'\n",
    "            occupations = people[occ_key] if occ_key in people else None\n",
    "            sample = InputSample(people['title'], people['summary'], occupations)\n",
    "            yield sample\n",
    "\n",
    "\n",
    "def gen_vocabulary(data_file, vocab_file):\n",
    "    '''\n",
    "    generate a word list given an input corpus\n",
    "    :param data_file:\n",
    "    :param vocab_file:\n",
    "    :return:\n",
    "    '''\n",
    "    vocab = set()\n",
    "    for sample in tqdm(load_data(data_file)):\n",
    "        sentence = str.lower(sample.summary)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        vocab.update(set(tokens))\n",
    "\n",
    "    with open(vocab_file, 'w', encoding='utf8')as f:\n",
    "        f.write('\\n'.join(list(vocab)))\n",
    "\n",
    "    print('done! The size of vocabulary is {a}.'.format(a=len(vocab)))\n",
    "\n",
    "\n",
    "def load_vocabulary(vocab_file):\n",
    "    '''\n",
    "    load vocabulary and create an id for each token.\n",
    "    <pad> means padding token, <unk> means unknown token\n",
    "    :param vocab_file:\n",
    "    :return:\n",
    "    '''\n",
    "    vocab_to_id = dict()\n",
    "    with open(vocab_file, encoding='utf8')as f:\n",
    "        words = f.readlines()\n",
    "        for w_id, word in enumerate(words):\n",
    "            word = word.replace('\\n', '')\n",
    "            vocab_to_id[word] = w_id+1\n",
    "    vocab_to_id['<pad>'] = 0\n",
    "    vocab_to_id['<unk>'] = len(vocab_to_id)\n",
    "    return vocab_to_id\n",
    "\n",
    "\n",
    "def read_dataset(data_file, vocab_to_id, sent_len, debug=False):\n",
    "    '''\n",
    "    read training set or test set\n",
    "    :param data_file:\n",
    "    :param vocab_to_id:\n",
    "    :param sent_len: the\n",
    "    :param debug: load only a small fraction of samples to debug\n",
    "    :return: model's input and labels\n",
    "    need about 1min31s for training set and 2min for test set\n",
    "    '''\n",
    "\n",
    "    labels, _ = get_label()\n",
    "    unknown_id = len(vocab_to_id) - 1\n",
    "    data_x, data_y = list(), list()\n",
    "    cnt = 0\n",
    "    \n",
    "    for sample in tqdm(load_data(data_file)):\n",
    "        # print(sample)\n",
    "\n",
    "        # for debugging\n",
    "        cnt += 1\n",
    "        if debug and cnt > 100:\n",
    "            break\n",
    "\n",
    "        summary = str.lower(sample.summary)\n",
    "        tokens = nltk.word_tokenize(summary)\n",
    "        token_ids = [vocab_to_id.get(t, unknown_id) for t in tokens]\n",
    "        token_ids = pad_sentence(token_ids, sent_len)\n",
    "        data_x.append(token_ids)\n",
    "        occupations = sample.occupation\n",
    "\n",
    "        # train\n",
    "        if occupations:\n",
    "            y_vector = [1 if label in occupations else 0 for label in labels]\n",
    "            data_y.append(y_vector)\n",
    "        # test\n",
    "        else:\n",
    "            data_y.append(0)\n",
    "\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "\n",
    "def f1_score(true_labels, pred_labels):\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    nb_correct, nb_pred, nb_true = 0, 0, 0\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        nb_correct += len(true & pred)\n",
    "        nb_pred += len(pred)\n",
    "        nb_true += len(true)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score, p, r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules and methods\n",
    "\n",
    "# Import functions from keras\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from  keras import Sequential\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "# Import some basic packages\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "# [ train_file ] is a training dataset that contains 266K samples.\n",
    "# [ test_file ] is a testing dataset that contains 200K samples. You can test your model based on this file.\n",
    "# [ predict_file ] is a predicting dataset that contains 201K samples. Each sample in this file does not have occupation labels.\n",
    "\n",
    "train_file = 'wiki-train.json.gz'\n",
    "test_file = 'wiki-dev.json.gz'\n",
    "predict_file = 'wiki-test.json.gz'\n",
    "\n",
    "# output files\n",
    "# [ vocab_file ] has a word vocabulary that defines which words participate in this task.\n",
    "# The default vocabulary is generated by our methods from training dataset,\n",
    "# but you can create it in a way you like.\n",
    "# [ model_file ] is used for store your trained model\n",
    "# [ result_file ] is file that stores your predicted occupations.\n",
    "# (This is the file you have to submit, once you ran on the test dataset)\n",
    "\n",
    "vocab_file = 'vocab.txt'\n",
    "model_file = 'my_model.h5'\n",
    "result_file = 'result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters: You don't have to change these, but you can.\n",
    "# [ embedding_dimension ] the dimensions of word embeddings\n",
    "# [ maximal_sentence_length ] the maximum length of each sentence\n",
    "# [ number_of_labels ] the number of occupations\n",
    "# [ epochs ] training epochs. Adjust this parameters to avoid overfitting and underfitting.\n",
    "# [ batch_size ] the number of samples. It determines how many samples would be fed into your model.\n",
    "# The size of this parameter also depends on how good your hardware is.\n",
    "# [ theta ] A threshold to determine whether to assign a specific occupation label given a input sample.\n",
    "# A suitable theta value will help your model\n",
    "\n",
    "embedding_dimension = 200\n",
    "maximal_sentence_length = 100\n",
    "number_of_labels = 20\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "theta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    DefaultConv2D = partial(keras.layers.Conv2D,kernel_size=3, activation=\\'relu\\', padding=\"SAME\")\\n\\n    model = Sequential()\\n    # initialized word embeddings randomly\\n    \\n    embedding_matrix = Word2Vec(vocab, vector_size=maximal_sentence_length, window=5, min_count=1, workers=4)\\n   \\n    #model.add(Embedding(len(vocab), embedding_dimension, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable = False))\\n    model.add(Embedding(len(vocab), embedding_dimension, input_length=maximal_sentence_length))\\n\\n    #model.add(GlobalAveragePooling1D())\\n    # two layer densely-connected NNs\\n    # # different parameters of a dense layer can have different performances\\n    # # here, we let the number of units equal 32, and use a relu activation.\\n    #model.add(Dense(32, activation=\\'relu\\'))\\n    #model.add(DefaultConv2D(filters = 32, kernel_size = 7, input_shape = []))\\n    #model.add(MaxPooling2D(pool_size=2))\\n    #model.add(Dropout(0.25))\\n    #model.add(Flatten())\\n    \\n    model.add(keras.layers.GRU(128, return_sequences = True))\\n    model.add(keras.layers.GRU(128))\\n    #model.add(keras.layers.GRU(128))\\n\\n    model.add(Dense(number_of_labels, activation=\"sigmoid\"))\\n    \\n\\n    # compile the model\\n    model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'sgd\\', metrics=[\\'binary_accuracy\\'])\\n\\n    model = keras.Sequential([\\n        hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", \\n                       dtype=\"string\", input_shape=[100], output_shape=[50]),\\n        keras.layers.Dense(128, activation=\"relu\"),\\n        keras.layers.Dense(number_of_labels, activation=\"sigmoid\")\\n])\\n              \\n\\n              inputs = Input(shape=(sequence_length,))\\nembedding = embedding_layer(inputs)\\nreshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\\n\\nconv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation=\\'relu\\',kernel_regularizer=regularizers.l2(0.01))(reshape)\\nconv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation=\\'relu\\',kernel_regularizer=regularizers.l2(0.01))(reshape)\\n\\nmaxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\\nmaxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\\n\\nmerged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\\nflatten = Flatten()(merged_tensor)\\nreshape = Reshape((2*num_filters,))(flatten)\\ndropout = Dropout(drop)(flatten)\\nconc = Dense(40)(dropout)\\noutput = Dense(units=6, activation=\\'sigmoid\\',kernel_regularizer=regularizers.l2(0.01))(conc)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "from functools import partial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def create_model(vocab):\n",
    "    '''\n",
    "    :param vocab: a vocabulary dictionary which looks like {'python':0, 'java':1 ......}\n",
    "    :return:\n",
    "    '''\n",
    "    encoder = keras.models.Sequential([\n",
    "        keras.layers.Embedding(len(vocab), embedding_dimension, input_length=maximal_sentence_length),\n",
    "        keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "    decoder = keras.models.Sequential([\n",
    "        keras.layers.LSTM(128, return_sequences=True),\n",
    "        keras.layers.Dense(number_of_labels, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        encoder,\n",
    "        decoder\n",
    "])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"binary_accuracy\"])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "    DefaultConv2D = partial(keras.layers.Conv2D,kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "    model = Sequential()\n",
    "    # initialized word embeddings randomly\n",
    "    \n",
    "    embedding_matrix = Word2Vec(vocab, vector_size=maximal_sentence_length, window=5, min_count=1, workers=4)\n",
    "   \n",
    "    #model.add(Embedding(len(vocab), embedding_dimension, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable = False))\n",
    "    model.add(Embedding(len(vocab), embedding_dimension, input_length=maximal_sentence_length))\n",
    "\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    # two layer densely-connected NNs\n",
    "    # # different parameters of a dense layer can have different performances\n",
    "    # # here, we let the number of units equal 32, and use a relu activation.\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    #model.add(DefaultConv2D(filters = 32, kernel_size = 7, input_shape = []))\n",
    "    #model.add(MaxPooling2D(pool_size=2))\n",
    "    #model.add(Dropout(0.25))\n",
    "    #model.add(Flatten())\n",
    "    \n",
    "    model.add(keras.layers.GRU(128, return_sequences = True))\n",
    "    model.add(keras.layers.GRU(128))\n",
    "    #model.add(keras.layers.GRU(128))\n",
    "\n",
    "    model.add(Dense(number_of_labels, activation=\"sigmoid\"))\n",
    "    \n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", \n",
    "                       dtype=\"string\", input_shape=[100], output_shape=[50]),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dense(number_of_labels, activation=\"sigmoid\")\n",
    "])\n",
    "              \n",
    "\n",
    "              inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((2*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "conc = Dense(40)(dropout)\n",
    "output = Dense(units=6, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(conc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(debug):\n",
    "    '''\n",
    "    train your model.\n",
    "    :param debug:whether to use a small fraction of samples\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prepare data\n",
    "    vocab_to_id = load_vocabulary(vocab_file)\n",
    "    data_x, data_y = read_dataset(train_file, vocab_to_id, maximal_sentence_length, debug=debug)\n",
    "\n",
    "    # create a model\n",
    "    model = create_model(vocab_to_id)\n",
    "    model.summary()\n",
    "\n",
    "    # train\n",
    "    print('start to train, data size = {a}'.format(a=len(data_x)))\n",
    "    model.fit(data_x, data_y, validation_split=0.10, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # save model\n",
    "    model.save(model_file)\n",
    "\n",
    "\n",
    "def evaluate_on_dev(debug):\n",
    "    '''\n",
    "    evaluare your model on the development dataset.\n",
    "\n",
    "    :param debug:whether to use a small fraction of samples\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prepare data\n",
    "    vocab_to_id = load_vocabulary(vocab_file)\n",
    "    data_x, data_y = read_dataset(test_file, vocab_to_id, maximal_sentence_length, debug=debug)\n",
    "    raw_samples = list(load_data(test_file))\n",
    "    print('start to do validation, data size = {a}'.format(a=len(data_x)))\n",
    "    _, id_to_labels = get_label()\n",
    "    pred_labels, true_labels = list(), list()\n",
    "\n",
    "    # load model\n",
    "    model = load_model(model_file)\n",
    "\n",
    "    # predict each sample\n",
    "    for summary, label, raw in zip(data_x, data_y, raw_samples):\n",
    "        result = model.predict(np.array([summary]))[0]\n",
    "        pred = set([id_to_labels[i] for i, prob in enumerate(result) if prob > theta])\n",
    "        true = set([id_to_labels[index] for index, e in enumerate(label) if e == 1])\n",
    "        pred_labels.append(pred)\n",
    "        true_labels.append(true)\n",
    "\n",
    "        # print wrong prediction\n",
    "        print('Title:' + raw.title)\n",
    "        wrong_occupations = pred - true\n",
    "        if len(wrong_occupations) > 0:\n",
    "            print('[ wrong prediction ] this person does not have the occupations:{a}'.format(a=wrong_occupations))\n",
    "        missing_occupations = true - pred\n",
    "        if len(missing_occupations) > 0:\n",
    "            print('[ missing prediction ] your prediction miss the occupations:{b}'.format(b=missing_occupations))\n",
    "        print('---------------------------')\n",
    "\n",
    "    # calculate metrics\n",
    "    f1, precision, recall = f1_score(true_labels, pred_labels)\n",
    "    print('result on validation set, f1 : {a}, precision : {b}, recall : {c}.'.\n",
    "          format(a=f1, b=precision, c=recall))\n",
    "\n",
    "\n",
    "def predict_on_test(debug):\n",
    "    '''\n",
    "    :param debug: whether to use a small fraction of samples\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # prepare data\n",
    "    _, id_to_labels = get_label()\n",
    "    vocab_to_id = load_vocabulary(vocab_file)\n",
    "    model = load_model(model_file)\n",
    "    datax, _, = read_dataset(predict_file, vocab_to_id, maximal_sentence_length, debug)\n",
    "    raw_samples = list(load_data(predict_file))\n",
    "\n",
    "    # predict\n",
    "    r_f = open(result_file, 'w', encoding='utf8')\n",
    "    for data, raw_sample in tqdm(zip(datax, raw_samples)):\n",
    "        result = model.predict(np.array([data]))[0]\n",
    "        pred = [id_to_labels[i] for i, prob in enumerate(result) if prob > theta]\n",
    "        r_f.write(json.dumps({\n",
    "                            'title': raw_sample.title,\n",
    "                            'occupations': pred\n",
    "                        }) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1675.82it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer sequential_7 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4988/373090386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Change 'debug' to False when your want to train and test on all samples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdebug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mevaluate_on_dev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpredict_on_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4988/2445469957.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# create a model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_to_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4988/259905429.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(vocab)\u001b[0m\n\u001b[0;32m     20\u001b[0m ])\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     model = keras.models.Sequential([\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    215\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    977\u001b[0m                                                 input_list)\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[0;32m   1113\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[0;32m   1115\u001b[0m           inputs, input_masks, args, kwargs)\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    884\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m           \u001b[1;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m     \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2633\u001b[1;33m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[0;32m   2634\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[0;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    215\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer sequential_7 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# create a vocabulary file if does not exist\n",
    "if not os.path.exists(vocab_file):\n",
    "    gen_vocabulary(train_file, vocab_file)\n",
    "\n",
    "# train & evaluate & predict\n",
    "# note: the switch 'debug' is True means only using a small fraction of samples, which can save time to debug your code.\n",
    "# Change 'debug' to False when your want to train and test on all samples.\n",
    "debug = True\n",
    "train(debug=debug)\n",
    "evaluate_on_dev(debug=debug)\n",
    "predict_on_test(debug=debug)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
